{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002522d0-cb97-4e37-93e8-6d849b8b6a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lendo a tabela trusted_associado...\n",
      "Lendo a dimensão dim_associado existente...\n",
      "Calculando registros para INSERT e UPDATE...\n",
      "Criando cache para otimização...\n",
      "Nenhum registro para atualizar.\n",
      "Nenhum registro novo para inserir.\n",
      "Processamento da dimensão Associado concluído com sucesso.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import psycopg2 \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, when, current_date, xxhash64, abs, max\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DateType, BooleanType, LongType, IntegerType\n",
    "\n",
    "# Define o caminho do JAR como uma constante para ser reutilizado\n",
    "JAR_PATH = \"/opt/spark/jars/postgresql-42.7.3.jar\"\n",
    "\n",
    "def get_spark_session():\n",
    "    \"\"\"\n",
    "    Cria e retorna uma SparkSession com a configuração JDBC para PostgreSQL.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        SparkSession.builder.appName(\"ETL Refined Layer - Dimension Creation with Spark SQL\")\n",
    "        .config(\"spark.jars\", JAR_PATH)\n",
    "        .getOrCreate()\n",
    "    )\n",
    "\n",
    "def execute_postgres_update(db_details: dict, temp_table_name: str):\n",
    "    \"\"\"\n",
    "    Conecta-se ao PostgreSQL.\n",
    "    \"\"\"\n",
    "    conn = None\n",
    "    try:\n",
    "        # Conecta ao banco de dados usando os detalhes extraídos da URL JDBC\n",
    "        conn = psycopg2.connect(\n",
    "            dbname=db_details[\"dbname\"],\n",
    "            user=db_details[\"user\"],\n",
    "            password=db_details[\"password\"],\n",
    "            host=db_details[\"host\"],\n",
    "            port=db_details[\"port\"]\n",
    "        )\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        # Query de UPDATE\n",
    "        update_query = f\"\"\"\n",
    "            UPDATE dim_associado\n",
    "            SET fim_vigencia_registro_associado = current_timestamp,\n",
    "                registro_ativo_associado = false\n",
    "            WHERE sk_associado IN (SELECT sk_associado FROM {temp_table_name});\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"Executando o UPDATE no banco de dados...\")\n",
    "        cur.execute(update_query)\n",
    "        rows_affected = cur.rowcount\n",
    "        print(f\"Comando executado com sucesso. Linhas afetadas: {rows_affected}\")\n",
    "        \n",
    "        # Efetiva a transação\n",
    "        conn.commit()\n",
    "        cur.close()\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(f\"Ocorreu um erro ao executar o UPDATE via psycopg2: {error}\")\n",
    "        if conn is not None:\n",
    "            conn.rollback() \n",
    "        raise error\n",
    "    finally:\n",
    "        if conn is not None:\n",
    "            conn.close()\n",
    "\n",
    "def process_dim_associado_sql(spark: SparkSession):\n",
    "    \"\"\"\n",
    "    Lê a tabela trusted_associado e atualiza a dimensão dim_associado.\n",
    "    \"\"\"\n",
    "\n",
    "    # Configuração de Conexão com o DW\n",
    "    db_url = \"jdbc:postgresql://db:5432/SiCooperativeDW\"\n",
    "    db_properties = {\n",
    "        \"user\": \"user\",\n",
    "        \"password\": \"password\",\n",
    "        \"driver\": \"org.postgresql.Driver\",\n",
    "    }\n",
    "    \n",
    "    # Leitura dos dados\n",
    "    print(\"Lendo a tabela trusted_associado...\")\n",
    "    df_trusted_associado = spark.read.jdbc(url=db_url, table=\"trusted_associado\", properties=db_properties)\n",
    "    \n",
    "    try:\n",
    "        print(\"Lendo a dimensão dim_associado existente...\")\n",
    "        df_dim_associado = spark.read.jdbc(url=db_url, table=\"dim_associado\", properties=db_properties)\n",
    "    except Exception as e:\n",
    "        print(f\"Tabela dim_associado não encontrada. A primeira carga será feita como 'novas_inclusoes'.\")\n",
    "        dim_schema = StructType([\n",
    "            StructField(\"sk_associado\", LongType(), False), StructField(\"id_associado\", StringType(), True),\n",
    "            StructField(\"nome_associado\", StringType(), True), StructField(\"sobrenome_associado\", StringType(), True), \n",
    "            StructField(\"data_nascimento_associado\", DateType(), True),\n",
    "            StructField(\"idade_atual_associado\", IntegerType(), True), StructField(\"estado_civil_associado\", StringType(), True), \n",
    "            StructField(\"escolaridade_associado\", StringType(), True), StructField(\"inicio_vigencia_registro_associado\", DateType(), True),\n",
    "            StructField(\"fim_vigencia_registro_associado\", DateType(), True), StructField(\"registro_ativo_associado\", BooleanType(), True)\n",
    "        ])\n",
    "        df_dim_associado = spark.createDataFrame([], schema=dim_schema)\n",
    "        \n",
    "    # Criação das views temporarias\n",
    "    df_trusted_associado.createOrReplaceTempView(\"source_trusted_associado\")\n",
    "    df_dim_associado.createOrReplaceTempView(\"target_dim_associado\")\n",
    "    \n",
    "    # CTE para identificar os registros a serem inseridos (novos e alterados)\n",
    "    records_to_insert_sql = \"\"\"\n",
    "        WITH\n",
    "        novas_inclusoes AS (\n",
    "            SELECT \n",
    "            CAST(abs(xxhash64(id_associado, current_timestamp())) % 10000000000 AS BIGINT) AS sk_associado, \n",
    "            id_associado, \n",
    "            nome as nome_associado, \n",
    "            sobrenome as sobrenome_associado, \n",
    "            data_nascimento as data_nascimento_associado, \n",
    "            idade_atual_associado, \n",
    "            estado_civil as estado_civil_associado, \n",
    "            escolaridade as escolaridade_associado, \n",
    "            current_timestamp() AS inicio_vigencia_registro_associado, \n",
    "            CAST(NULL AS TIMESTAMP) AS fim_vigencia_registro_associado, \n",
    "            true AS registro_ativo_associado\n",
    "            FROM source_trusted_associado ta\n",
    "            WHERE NOT EXISTS (\n",
    "                                SELECT 1\n",
    "                                FROM target_dim_associado da\n",
    "                                WHERE\n",
    "                                    da.id_associado = ta.id_associado\n",
    "                                    AND da.estado_civil_associado = ta.estado_civil\n",
    "                                    AND da.escolaridade_associado = ta.escolaridade\n",
    "                                    AND da.idade_atual_associado = ta.idade_atual_associado\n",
    "            ))\n",
    "            SELECT * FROM novas_inclusoes \n",
    "        \"\"\"\n",
    "    \n",
    "    # CTE para identificar os registros antigos que precisam ser desativados (UPDATE)\n",
    "    records_to_update_sql = \"\"\"\n",
    "        WITH atualizados AS (\n",
    "            SELECT \n",
    "                da.sk_associado\n",
    "            FROM source_trusted_associado ta \n",
    "            INNER JOIN target_dim_associado da ON ta.id_associado = da.id_associado AND da.registro_ativo_associado = TRUE\n",
    "             WHERE NOT EXISTS (\n",
    "                                SELECT 1\n",
    "                                FROM target_dim_associado da\n",
    "                                WHERE\n",
    "                                    da.id_associado = ta.id_associado\n",
    "                                    AND da.estado_civil_associado = ta.estado_civil\n",
    "                                    AND da.escolaridade_associado = ta.escolaridade\n",
    "                                    AND da.idade_atual_associado = ta.idade_atual_associado\n",
    "        ))\n",
    "        SELECT sk_associado FROM atualizados\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Calculando registros para INSERT e UPDATE...\")\n",
    "    df_to_insert = spark.sql(records_to_insert_sql)\n",
    "    df_to_update = spark.sql(records_to_update_sql)\n",
    "\n",
    "    print(\"Criando cache para otimização...\")\n",
    "    df_to_insert.cache()\n",
    "    df_to_update.cache()\n",
    "\n",
    "    if not df_to_update.rdd.isEmpty():\n",
    "        temp_update_table = \"staging_updates_associado\"\n",
    "        print(f\"Encontrados {df_to_update.count()} registros para desativar (UPDATE)...\")\n",
    "        print(f\"Escrevendo chaves em uma tabela temporária: {temp_update_table}\")\n",
    "        df_to_update.select(\"sk_associado\").write.jdbc(\n",
    "            url=db_url, table=temp_update_table, mode=\"overwrite\", properties=db_properties\n",
    "        )\n",
    "        \n",
    "        # Extrai detalhes da URL para a conexão com psycopg2\n",
    "        match = re.search(r\"postgresql://(.*?):(.*?)/(.*)\", db_url)\n",
    "        host, port, dbname = match.groups()\n",
    "        \n",
    "        db_details_for_update = {\n",
    "            \"user\": db_properties[\"user\"],\n",
    "            \"password\": db_properties[\"password\"],\n",
    "            \"host\": host,\n",
    "            \"port\": port,\n",
    "            \"dbname\": dbname\n",
    "        }\n",
    "        \n",
    "        execute_postgres_update(db_details_for_update, temp_update_table)\n",
    "        print(\"UPDATEs concluídos.\")\n",
    "    else:\n",
    "        print(\"Nenhum registro para atualizar.\")\n",
    "        \n",
    "    # --- ETAPA DE INSERT ---\n",
    "    if df_to_insert.count() > 0:\n",
    "        print(f\"Encontrados {df_to_insert.count()} novos registros para inserir (INSERT)...\")\n",
    "        df_to_insert.write.jdbc(\n",
    "            url=db_url, table=\"dim_associado\", mode=\"append\", properties=db_properties\n",
    "        )\n",
    "        print(\"INSERTs concluídos.\")\n",
    "    else:\n",
    "        print(\"Nenhum registro novo para inserir.\")\n",
    "\n",
    "    # Limpa o cache\n",
    "    df_to_insert.unpersist()\n",
    "    df_to_update.unpersist()\n",
    "\n",
    "    print(\"Processamento da dimensão Associado concluído com sucesso.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spark_session = get_spark_session()\n",
    "    process_dim_associado_sql(spark_session)\n",
    "    spark_session.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52af7bb-e5e4-403d-ad54-fe6039861a06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
