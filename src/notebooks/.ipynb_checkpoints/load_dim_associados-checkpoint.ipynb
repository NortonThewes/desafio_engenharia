{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "002522d0-cb97-4e37-93e8-6d849b8b6a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lendo a tabela trusted_associado...\n",
      "Lendo a dimensão dim_associado existente...\n",
      "Calculando registros para INSERT e UPDATE...\n"
     ]
    },
    {
     "ename": "ParseException",
     "evalue": "\n[PARSE_SYNTAX_ERROR] Syntax error at or near 'SELECT'.(line 26, pos 12)\n\n== SQL ==\n\n        WITH\n        novas_inclusoes AS (\n            SELECT \n            CAST(abs(xxhash64(id_associado, current_timestamp())) % 10000000000 AS BIGINT) AS sk_associado, \n            id_associado, \n            nome as nome_associado, \n            sobrenome as sobrenome_associado, \n            data_nascimento as data_nascimento_associado, \n            idade_atual_associado, \n            estado_civil as estado_civil_associado, \n            escolaridade as escolaridade_associado, \n            current_date() AS inicio_vigencia_registro_associado, \n            CAST(NULL AS DATE) AS fim_vigencia_registro_associado, \n            true AS registro_ativo_associado\n            FROM source_trusted_associado ta\n            WHERE NOT EXISTS (\n                                SELECT 1\n                                FROM target_dim_associado da\n                                WHERE\n                                    da.id_associado = ta.id_associado\n                                    AND da.estado_civil_associado = ta.estado_civil\n                                    AND da.escolaridade_associado = ta.escolaridade\n                                    AND da.idade_atual_associado = ta.idade_atual_associado\n            )\n            SELECT * FROM novas_inclusoes \n------------^^^\n        \n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParseException\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 204\u001b[0m\n\u001b[1;32m    202\u001b[0m spark_session \u001b[38;5;241m=\u001b[39m get_spark_session()\n\u001b[1;32m    203\u001b[0m \u001b[38;5;66;03m# A chamada para addJar foi removida daqui pois não é mais necessária\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m \u001b[43mprocess_dim_associado_sql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspark_session\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m spark_session\u001b[38;5;241m.\u001b[39mstop()\n",
      "Cell \u001b[0;32mIn[1], line 152\u001b[0m, in \u001b[0;36mprocess_dim_associado_sql\u001b[0;34m(spark)\u001b[0m\n\u001b[1;32m    133\u001b[0m records_to_update_sql \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;124m    WITH atualizados AS (\u001b[39m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124m        SELECT \u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;124m    SELECT sk_associado FROM atualizados\u001b[39m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalculando registros para INSERT e UPDATE...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 152\u001b[0m df_to_insert \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecords_to_insert_sql\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m df_to_update \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msql(records_to_update_sql)\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCriando cache para otimização...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mParseException\u001b[0m: \n[PARSE_SYNTAX_ERROR] Syntax error at or near 'SELECT'.(line 26, pos 12)\n\n== SQL ==\n\n        WITH\n        novas_inclusoes AS (\n            SELECT \n            CAST(abs(xxhash64(id_associado, current_timestamp())) % 10000000000 AS BIGINT) AS sk_associado, \n            id_associado, \n            nome as nome_associado, \n            sobrenome as sobrenome_associado, \n            data_nascimento as data_nascimento_associado, \n            idade_atual_associado, \n            estado_civil as estado_civil_associado, \n            escolaridade as escolaridade_associado, \n            current_date() AS inicio_vigencia_registro_associado, \n            CAST(NULL AS DATE) AS fim_vigencia_registro_associado, \n            true AS registro_ativo_associado\n            FROM source_trusted_associado ta\n            WHERE NOT EXISTS (\n                                SELECT 1\n                                FROM target_dim_associado da\n                                WHERE\n                                    da.id_associado = ta.id_associado\n                                    AND da.estado_civil_associado = ta.estado_civil\n                                    AND da.escolaridade_associado = ta.escolaridade\n                                    AND da.idade_atual_associado = ta.idade_atual_associado\n            )\n            SELECT * FROM novas_inclusoes \n------------^^^\n        \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import psycopg2 \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, when, current_date, xxhash64, abs, max\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DateType, BooleanType, LongType, IntegerType\n",
    "\n",
    "# Define o caminho do JAR como uma constante para ser reutilizado\n",
    "JAR_PATH = \"/opt/spark/jars/postgresql-42.7.3.jar\"\n",
    "\n",
    "def get_spark_session():\n",
    "    \"\"\"\n",
    "    Cria e retorna uma SparkSession com a configuração JDBC para PostgreSQL.\n",
    "    A configuração do driver extra foi removida pois não é mais necessária.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        SparkSession.builder.appName(\"ETL Refined Layer - Dimension Creation with Spark SQL\")\n",
    "        .config(\"spark.jars\", JAR_PATH)\n",
    "        .getOrCreate()\n",
    "    )\n",
    "\n",
    "def execute_postgres_update(db_details: dict, temp_table_name: str):\n",
    "    \"\"\"\n",
    "    Conecta-se ao PostgreSQL usando psycopg2 e executa um UPDATE\n",
    "    para desativar registros antigos. Esta abordagem é mais simples e robusta\n",
    "    do que usar Py4J para comandos DML.\n",
    "    \"\"\"\n",
    "    conn = None\n",
    "    try:\n",
    "        # Conecta ao banco de dados usando os detalhes extraídos da URL JDBC\n",
    "        conn = psycopg2.connect(\n",
    "            dbname=db_details[\"dbname\"],\n",
    "            user=db_details[\"user\"],\n",
    "            password=db_details[\"password\"],\n",
    "            host=db_details[\"host\"],\n",
    "            port=db_details[\"port\"]\n",
    "        )\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        # Query de UPDATE\n",
    "        update_query = f\"\"\"\n",
    "            UPDATE dim_associado\n",
    "            SET fim_vigencia_registro_associado = current_date,\n",
    "                registro_ativo_associado = false\n",
    "            WHERE sk_associado IN (SELECT sk_associado FROM {temp_table_name});\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"Executando o UPDATE no banco de dados...\")\n",
    "        cur.execute(update_query)\n",
    "        rows_affected = cur.rowcount\n",
    "        print(f\"Comando executado com sucesso. Linhas afetadas: {rows_affected}\")\n",
    "        \n",
    "        # Efetiva a transação\n",
    "        conn.commit()\n",
    "        cur.close()\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(f\"Ocorreu um erro ao executar o UPDATE via psycopg2: {error}\")\n",
    "        if conn is not None:\n",
    "            conn.rollback() \n",
    "        raise error\n",
    "    finally:\n",
    "        if conn is not None:\n",
    "            conn.close()\n",
    "\n",
    "def process_dim_associado_sql(spark: SparkSession):\n",
    "    \"\"\"\n",
    "    Lê a tabela trusted_associado e atualiza a dimensão dim_associado\n",
    "    aplicando a lógica de SCD Tipo 2 (INSERT/UPDATE) usando Spark SQL.\n",
    "    \"\"\"\n",
    "\n",
    "    # Configuração de Conexão com o DW\n",
    "    db_url = \"jdbc:postgresql://db:5432/SiCooperativeDW\"\n",
    "    db_properties = {\n",
    "        \"user\": \"user\",\n",
    "        \"password\": \"password\",\n",
    "        \"driver\": \"org.postgresql.Driver\",\n",
    "    }\n",
    "    \n",
    "    # Leitura dos dados\n",
    "    print(\"Lendo a tabela trusted_associado...\")\n",
    "    df_trusted_associado = spark.read.jdbc(url=db_url, table=\"trusted_associado\", properties=db_properties)\n",
    "    \n",
    "    try:\n",
    "        print(\"Lendo a dimensão dim_associado existente...\")\n",
    "        df_dim_associado = spark.read.jdbc(url=db_url, table=\"dim_associado\", properties=db_properties)\n",
    "    except Exception as e:\n",
    "        print(f\"Tabela dim_associado não encontrada. A primeira carga será feita como 'novas_inclusoes'.\")\n",
    "        dim_schema = StructType([\n",
    "            StructField(\"sk_associado\", LongType(), False), StructField(\"id_associado\", StringType(), True),\n",
    "            StructField(\"nome_associado\", StringType(), True), StructField(\"sobrenome_associado\", StringType(), True), \n",
    "            StructField(\"data_nascimento_associado\", DateType(), True),\n",
    "            StructField(\"idade_atual_associado\", IntegerType(), True), StructField(\"estado_civil_associado\", StringType(), True), \n",
    "            StructField(\"escolaridade_associado\", StringType(), True), StructField(\"inicio_vigencia_registro_associado\", DateType(), True),\n",
    "            StructField(\"fim_vigencia_registro_associado\", DateType(), True), StructField(\"registro_ativo_associado\", BooleanType(), True)\n",
    "        ])\n",
    "        df_dim_associado = spark.createDataFrame([], schema=dim_schema)\n",
    "        \n",
    "    # Criação das views temporarias\n",
    "    df_trusted_associado.createOrReplaceTempView(\"source_trusted_associado\")\n",
    "    df_dim_associado.createOrReplaceTempView(\"target_dim_associado\")\n",
    "    \n",
    "    # CTE para identificar os registros a serem inseridos (novos e alterados)\n",
    "    records_to_insert_sql = \"\"\"\n",
    "        WITH\n",
    "        novas_inclusoes AS (\n",
    "            SELECT \n",
    "            CAST(abs(xxhash64(id_associado, current_timestamp())) % 10000000000 AS BIGINT) AS sk_associado, \n",
    "            id_associado, \n",
    "            nome as nome_associado, \n",
    "            sobrenome as sobrenome_associado, \n",
    "            data_nascimento as data_nascimento_associado, \n",
    "            idade_atual_associado, \n",
    "            estado_civil as estado_civil_associado, \n",
    "            escolaridade as escolaridade_associado, \n",
    "            current_date() AS inicio_vigencia_registro_associado, \n",
    "            CAST(NULL AS DATE) AS fim_vigencia_registro_associado, \n",
    "            true AS registro_ativo_associado\n",
    "            FROM source_trusted_associado ta\n",
    "            WHERE NOT EXISTS (\n",
    "                                SELECT 1\n",
    "                                FROM target_dim_associado da\n",
    "                                WHERE\n",
    "                                    da.id_associado = ta.id_associado\n",
    "                                    AND da.estado_civil_associado = ta.estado_civil\n",
    "                                    AND da.escolaridade_associado = ta.escolaridade\n",
    "                                    AND da.idade_atual_associado = ta.idade_atual_associado\n",
    "            ))\n",
    "            SELECT * FROM novas_inclusoes \n",
    "        \"\"\"\n",
    "    \n",
    "    # CTE para identificar os registros antigos que precisam ser desativados (UPDATE)\n",
    "    records_to_update_sql = \"\"\"\n",
    "        WITH atualizados AS (\n",
    "            SELECT \n",
    "                da.sk_associado\n",
    "            FROM source_trusted_associado ta \n",
    "            INNER JOIN target_dim_associado da ON ta.id_associado = da.id_associado AND da.registro_ativo_associado = TRUE\n",
    "             WHERE NOT EXISTS (\n",
    "                                SELECT 1\n",
    "                                FROM target_dim_associado da\n",
    "                                WHERE\n",
    "                                    da.id_associado = ta.id_associado\n",
    "                                    AND da.estado_civil_associado = ta.estado_civil\n",
    "                                    AND da.escolaridade_associado = ta.escolaridade\n",
    "                                    AND da.idade_atual_associado = ta.idade_atual_associado\n",
    "        ))\n",
    "        SELECT sk_associado FROM atualizados\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Calculando registros para INSERT e UPDATE...\")\n",
    "    df_to_insert = spark.sql(records_to_insert_sql)\n",
    "    df_to_update = spark.sql(records_to_update_sql)\n",
    "\n",
    "    print(\"Criando cache para otimização...\")\n",
    "    df_to_insert.cache()\n",
    "    df_to_update.cache()\n",
    "\n",
    "    if not df_to_update.rdd.isEmpty():\n",
    "        temp_update_table = \"staging_updates_associado\"\n",
    "        print(f\"Encontrados {df_to_update.count()} registros para desativar (UPDATE)...\")\n",
    "        print(f\"Escrevendo chaves em uma tabela temporária: {temp_update_table}\")\n",
    "        df_to_update.select(\"sk_associado\").write.jdbc(\n",
    "            url=db_url, table=temp_update_table, mode=\"overwrite\", properties=db_properties\n",
    "        )\n",
    "        \n",
    "        # Extrai detalhes da URL para a conexão com psycopg2\n",
    "        match = re.search(r\"postgresql://(.*?):(.*?)/(.*)\", db_url)\n",
    "        host, port, dbname = match.groups()\n",
    "        \n",
    "        db_details_for_update = {\n",
    "            \"user\": db_properties[\"user\"],\n",
    "            \"password\": db_properties[\"password\"],\n",
    "            \"host\": host,\n",
    "            \"port\": port,\n",
    "            \"dbname\": dbname\n",
    "        }\n",
    "        \n",
    "        execute_postgres_update(db_details_for_update, temp_update_table)\n",
    "        print(\"UPDATEs concluídos.\")\n",
    "    else:\n",
    "        print(\"Nenhum registro para atualizar.\")\n",
    "        \n",
    "    # --- ETAPA DE INSERT ---\n",
    "    if df_to_insert.count() > 0:\n",
    "        print(f\"Encontrados {df_to_insert.count()} novos registros para inserir (INSERT)...\")\n",
    "        df_to_insert.write.jdbc(\n",
    "            url=db_url, table=\"dim_associado\", mode=\"append\", properties=db_properties\n",
    "        )\n",
    "        print(\"INSERTs concluídos.\")\n",
    "    else:\n",
    "        print(\"Nenhum registro novo para inserir.\")\n",
    "\n",
    "    # Limpa o cache\n",
    "    df_to_insert.unpersist()\n",
    "    df_to_update.unpersist()\n",
    "\n",
    "    print(\"Processamento da dimensão Associado concluído com sucesso.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spark_session = get_spark_session()\n",
    "    # A chamada para addJar foi removida daqui pois não é mais necessária\n",
    "    process_dim_associado_sql(spark_session)\n",
    "    spark_session.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52af7bb-e5e4-403d-ad54-fe6039861a06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
