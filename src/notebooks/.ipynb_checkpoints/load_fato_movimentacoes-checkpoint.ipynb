{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acade33a-043a-4742-bdd6-8d5a8eda518b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session criada com sucesso.\n",
      "Iniciando a leitura das tabelas e criação das views temporárias...\n",
      "Lendo tabela 'trusted_movimentacao_cartao' e registrando como view 'source_trusted_movimentacao'...\n",
      "Lendo tabela 'trusted_cartao' e registrando como view 'source_trusted_cartao'...\n",
      "Lendo tabela 'trusted_conta' e registrando como view 'source_trusted_conta'...\n",
      "Lendo tabela 'trusted_associado' e registrando como view 'source_trusted_associado'...\n",
      "Lendo tabela 'dim_cartao' e registrando como view 'target_dim_cartao'...\n",
      "Lendo tabela 'dim_conta' e registrando como view 'target_dim_conta'...\n",
      "Lendo tabela 'dim_associado' e registrando como view 'target_dim_associado'...\n",
      "Lendo tabela 'dim_data' e registrando como view 'target_dim_data'...\n",
      "Todas as tabelas foram carregadas como views temporárias.\n",
      "Executando a query de transformação com Spark SQL...\n",
      "\n",
      "--- INICIANDO TESTES DE QUALIDADE DE DADOS ---\n",
      "\n",
      "Teste 1: Verificando se a contagem de registros da fato bate com a origem...\n",
      "SUCESSO! Contagem de registros validada: 12 registros.\n",
      "\n",
      "Teste 2: Verificando se não há duplicidade na chave primária (id_movimentacao_cartao)...\n",
      "SUCESSO! Nenhuma chave primária duplicada encontrada.\n",
      "\n",
      "--- TODOS OS TESTES DE QUALIDADE DE DADOS PASSARAM COM SUCESSO! ---\n",
      "\n",
      "Iniciando a carga full na tabela 'fato_movimentacao_cartao'...\n",
      "Carga de 12 registros na tabela 'fato_movimentacao_cartao' concluída com sucesso!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Define o caminho do JAR como uma constante para ser reutilizado\n",
    "JAR_PATH = \"/opt/spark/jars/postgresql-42.7.3.jar\"\n",
    "\n",
    "# Configuração de Conexão com o Data Warehouse\n",
    "DB_URL = \"jdbc:postgresql://db:5432/SiCooperativeDW\"\n",
    "DB_PROPERTIES = {\n",
    "    \"user\": \"user\",\n",
    "    \"password\": \"password\",\n",
    "    \"driver\": \"org.postgresql.Driver\",\n",
    "}\n",
    "\n",
    "# Nomes das tabelas que serão lidas\n",
    "TABLES_TO_READ = {\n",
    "    # Tabelas Trusted\n",
    "    \"trusted_movimentacao_cartao\": \"source_trusted_movimentacao\",\n",
    "    \"trusted_cartao\": \"source_trusted_cartao\",\n",
    "    \"trusted_conta\": \"source_trusted_conta\",\n",
    "    \"trusted_associado\": \"source_trusted_associado\",\n",
    "    # Tabelas de Dimensão \n",
    "    \"dim_cartao\": \"target_dim_cartao\",\n",
    "    \"dim_conta\": \"target_dim_conta\",\n",
    "    \"dim_associado\": \"target_dim_associado\",\n",
    "    \"dim_data\": \"target_dim_data\"\n",
    "}\n",
    "\n",
    "# Tabela de destino\n",
    "FATO_MOVIMENTACAO_TABLE = \"fato_movimentacao_cartao\"\n",
    "\n",
    "\n",
    "def get_spark_session():\n",
    "    \"\"\"\n",
    "    Cria e retorna uma SparkSession com a configuração JDBC para PostgreSQL.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        SparkSession.builder.appName(\"ETL Fato Movimentacao - Full Load with Spark SQL\")\n",
    "        .config(\"spark.jars\", JAR_PATH)\n",
    "        .getOrCreate()\n",
    "    )\n",
    "\n",
    "def run_data_quality_tests(spark, fato_df):\n",
    "    \"\"\"\n",
    "    Executa testes de qualidade de dados no DataFrame da fato antes da carga.\n",
    "    Levanta uma exceção se algum teste falhar.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- INICIANDO TESTES DE QUALIDADE DE DADOS ---\")\n",
    "    \n",
    "    # Contagem de Registros ---\n",
    "    print(\"\\nTeste 1: Verificando se a contagem de registros da fato bate com a origem...\")\n",
    "    \n",
    "    # Contagem de registros da tabela trusted\n",
    "    source_count = spark.table(\"source_trusted_movimentacao\").count()\n",
    "    # Contagem de registros do DataFrame da fato que foi gerada\n",
    "    target_count = fato_df.count()\n",
    "\n",
    "    # Se as contagens não baterem, o teste falha.\n",
    "    if source_count != target_count:\n",
    "        error_message = (\n",
    "            f\"FALHA NO TESTE DE CONTAGEM! \"\n",
    "            f\"Origem ('trusted_movimentacao_cartao') tem {source_count} registros. \"\n",
    "            f\"Destino ('fato_movimentacao_cartao') gerou {target_count} registros.\"\n",
    "        )\n",
    "        print(error_message)\n",
    "        raise ValueError(error_message)\n",
    "    else:\n",
    "        print(f\"SUCESSO! Contagem de registros validada: {source_count} registros.\")\n",
    "        \n",
    "    # Verificação de Chave Primária Única\n",
    "    print(\"\\nTeste 2: Verificando se não há duplicidade na chave primária (id_movimentacao_cartao)...\")\n",
    "    \n",
    "    # Conta o total de registros\n",
    "    total_records = target_count\n",
    "    # Conta o total de registros distintos pela chave primária\n",
    "    distinct_pk_count = fato_df.select(\"id_movimentacao_cartao\").distinct().count()\n",
    "\n",
    "    # Se a contagem total for diferente da contagem de chaves únicas, há duplicatas.\n",
    "    if total_records != distinct_pk_count:\n",
    "        duplicates = total_records - distinct_pk_count\n",
    "        error_message = (\n",
    "            f\"FALHA NO TESTE DE DUPLICIDADE! \"\n",
    "            f\"A tabela fato gerada tem {duplicates} chave(s) primária(s) duplicada(s).\"\n",
    "        )\n",
    "        print(error_message)\n",
    "        raise ValueError(error_message)\n",
    "    else:\n",
    "        print(f\"SUCESSO! Nenhuma chave primária duplicada encontrada.\")\n",
    "    \n",
    "    print(\"\\n--- TODOS OS TESTES DE QUALIDADE DE DADOS PASSARAM COM SUCESSO! ---\\n\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Função principal que executa o processo de ETL.\n",
    "    \"\"\"\n",
    "    spark = get_spark_session()\n",
    "    print(\"Spark Session criada com sucesso.\")\n",
    "\n",
    "    # Carrega todas as tabelas necessárias e as registra como views temporárias\n",
    "    print(\"Iniciando a leitura das tabelas e criação das views temporárias...\")\n",
    "    for table_name, view_name in TABLES_TO_READ.items():\n",
    "        print(f\"Lendo tabela '{table_name}' e registrando como view '{view_name}'...\")\n",
    "        df = spark.read.jdbc(url=DB_URL, table=table_name, properties=DB_PROPERTIES)\n",
    "        df.createOrReplaceTempView(view_name)\n",
    "    \n",
    "    print(\"Todas as tabelas foram carregadas como views temporárias.\")\n",
    "    \n",
    "    # Query lógica da contrução tabela fato\n",
    "    fato_movimentacao_sql = \"\"\"\n",
    "        WITH movimentacoes_enriquecidas AS (\n",
    "            SELECT \n",
    "                tmc.id_movimentacao_cartao,\n",
    "                tcr.id_cartao,\n",
    "                tcn.id_conta,\n",
    "                tas.id_associado,\n",
    "                tmc.valor_movimentacao,\n",
    "                tmc.data_movimentacao \n",
    "            FROM \n",
    "                source_trusted_movimentacao AS tmc \n",
    "            INNER JOIN \n",
    "                source_trusted_cartao AS tcr ON tmc.id_cartao = tcr.id_cartao \n",
    "            INNER JOIN \n",
    "                source_trusted_conta AS tcn ON tcr.id_conta = tcn.id_conta \n",
    "            INNER JOIN \n",
    "                source_trusted_associado AS tas ON tcn.id_associado = tas.id_associado\n",
    "        )\n",
    "        SELECT \n",
    "            men.id_movimentacao_cartao,\n",
    "            das.sk_associado,\n",
    "            dcr.sk_cartao,\n",
    "            dco.sk_conta,\n",
    "            ddt.sk_data,\n",
    "            men.valor_movimentacao,\n",
    "            men.data_movimentacao,\n",
    "            current_timestamp as data_hora_atualizacao_registro\n",
    "        FROM \n",
    "            movimentacoes_enriquecidas AS men\n",
    "        LEFT JOIN\n",
    "            target_dim_cartao AS dcr ON men.id_cartao = dcr.id_cartao\n",
    "        LEFT JOIN\n",
    "            target_dim_conta AS dco ON men.id_conta = dco.id_conta\n",
    "        LEFT JOIN\n",
    "            target_dim_associado AS das ON men.id_associado = das.id_associado\n",
    "                                AND men.data_movimentacao >= das.inicio_vigencia_registro_associado\n",
    "                                AND (men.data_movimentacao <= das.fim_vigencia_registro_associado OR das.fim_vigencia_registro_associado IS NULL)\n",
    "        LEFT JOIN\n",
    "            target_dim_data AS ddt ON CAST(men.data_movimentacao AS DATE) = ddt.`data`\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Executando a query de transformação com Spark SQL...\")\n",
    "    fato_df = spark.sql(fato_movimentacao_sql)\n",
    "    \n",
    "    # Antes de consolidar os dados, executa os testes.\n",
    "    run_data_quality_tests(spark, fato_df)\n",
    "\n",
    "    # Escrevendo o DataFrame final na tabela fato, sobrescrevendo os dados existentes.\n",
    "    print(f\"Iniciando a carga full na tabela '{FATO_MOVIMENTACAO_TABLE}'...\")\n",
    "    \n",
    "    fato_df.write.jdbc(\n",
    "        url=DB_URL,\n",
    "        table=FATO_MOVIMENTACAO_TABLE,\n",
    "        mode=\"overwrite\", \n",
    "        properties=DB_PROPERTIES\n",
    "    )\n",
    "    \n",
    "    print(f\"Carga de {fato_df.count()} registros na tabela '{FATO_MOVIMENTACAO_TABLE}' concluída com sucesso!\")\n",
    "\n",
    "    spark.stop()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5020e1c8-0b5b-4206-8a7c-7b332d69fc35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
